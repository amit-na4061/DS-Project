{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83855ad7-f6b9-4984-859e-c19f414f2ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from tqdm import tqdm  # library used for simple and convenient way to add progress bars to loops and iterable objects\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score,classification_report\n",
    "import re\n",
    "import gensim\n",
    "from nltk import sent_tokenize\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1dbe9cb9-c8ee-4248-b54a-4fca36efd169",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>statement</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>oh my gosh</td>\n",
       "      <td>Anxiety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>trouble sleeping, confused mind, restless hear...</td>\n",
       "      <td>Anxiety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All wrong, back off dear, forward doubt. Stay ...</td>\n",
       "      <td>Anxiety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I've shifted my focus to something else but I'...</td>\n",
       "      <td>Anxiety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I'm restless and restless, it's been a month n...</td>\n",
       "      <td>Anxiety</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           statement   status\n",
       "0                                         oh my gosh  Anxiety\n",
       "1  trouble sleeping, confused mind, restless hear...  Anxiety\n",
       "2  All wrong, back off dear, forward doubt. Stay ...  Anxiety\n",
       "3  I've shifted my focus to something else but I'...  Anxiety\n",
       "4  I'm restless and restless, it's been a month n...  Anxiety"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Combined Data.csv')\n",
    "df = df.iloc[:,1:]\n",
    "df = df[df['statement'].isnull()==False]\n",
    "df = df.reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc63ac04-cc2a-4f2c-813a-47dfd3c27fbd",
   "metadata": {},
   "source": [
    "#  Word 2 Vec Implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8016156-7685-4bba-890e-d5e433f59948",
   "metadata": {},
   "outputs": [],
   "source": [
    "## pip install gensim  ##need to install this library if you have not already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b26436c-8bbb-48fa-8b70-c3c3dce1519d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer=WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def process_text_lem(text):\n",
    "    'Applying lemmetization and removing english stop words'\n",
    "    cleaned_text = re.sub('[^a-zA-Z]', \" \", text)\n",
    "    cleaned_text = cleaned_text.lower()\n",
    "    words = cleaned_text.split()\n",
    "    processed_words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    processed_text = \" \".join(processed_words)\n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87cf7de3-3091-4395-9d77-c0dab5a6b1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10000\n",
    "corpus = []\n",
    "\n",
    "for start in range(0, len(df), batch_size):\n",
    "    end = start + batch_size\n",
    "    batch = df.iloc[start:end]\n",
    "    batch['processed_text'] = batch['statement'].apply(process_text_lem)\n",
    "    corpus.extend(batch['processed_text'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05b2b6ec-fcbd-4a81-9faf-928adc80b6cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 52681/52681 [00:04<00:00, 11228.86it/s]\n"
     ]
    }
   ],
   "source": [
    "# Lists to store indices and tokens\n",
    "empty_token_indices = []  # List to store indices of empty tokens\n",
    "words = []  # List to store non-empty tokens\n",
    "appended_indices = []  # List to store indices of corpus items appended to words\n",
    "\n",
    "for i, sent in enumerate(tqdm(corpus)):\n",
    "    sent_token = sent_tokenize(sent)\n",
    "    for sent in sent_token:\n",
    "        tokens = simple_preprocess(sent, min_len=2, max_len=15)\n",
    "        if not tokens:  # If the token list is empty\n",
    "            empty_token_indices.append(i)  # Store the index\n",
    "        else:\n",
    "            words.append(tokens)\n",
    "            appended_indices.append(i)  # Store the index of the original corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa979179-d51f-4714-b275-54319f4e5cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert appended_indices to a set for faster lookup\n",
    "indices_to_keep = set(appended_indices)\n",
    "# Filter the DataFrame to keep only rows whose index is in appended_indices\n",
    "df_filtered = df[df.index.isin(indices_to_keep)]\n",
    "# Alternatively, if you prefer to remove the rows not in appended_indices:\n",
    "#df_filtered = df.drop(df.index.difference(appended_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64b28e8f-4232-49b4-b9fe-3b0bb0e3a6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the word 2 Vec from scratch\n",
    "model_w2v = gensim.models.Word2Vec(words, window=7,min_count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66e35809-9d9e-4e57-8c11-e309faf7968c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['like',\n",
       " 'feel',\n",
       " 'want',\n",
       " 'know',\n",
       " 'life',\n",
       " 'get',\n",
       " 'time',\n",
       " 'even',\n",
       " 'people',\n",
       " 'would',\n",
       " 'day',\n",
       " 'year',\n",
       " 'thing',\n",
       " 'really',\n",
       " 'one',\n",
       " 'cannot',\n",
       " 'going',\n",
       " 'think',\n",
       " 'go',\n",
       " 'friend',\n",
       " 'make',\n",
       " 'much',\n",
       " 'never',\n",
       " 'help',\n",
       " 'feeling',\n",
       " 'could',\n",
       " 'work',\n",
       " 'thought',\n",
       " 'anymore',\n",
       " 'anxiety',\n",
       " 'back',\n",
       " 'anything',\n",
       " 'way',\n",
       " 'take',\n",
       " 'still',\n",
       " 'depression',\n",
       " 'something',\n",
       " 'good',\n",
       " 'got',\n",
       " 'always',\n",
       " 'everything',\n",
       " 'anyone',\n",
       " 'need',\n",
       " 'better',\n",
       " 'every',\n",
       " 'see',\n",
       " 'nothing',\n",
       " 'month',\n",
       " 'someone',\n",
       " 'also',\n",
       " 'family',\n",
       " 'bad',\n",
       " 'since',\n",
       " 'job',\n",
       " 'hate',\n",
       " 'right',\n",
       " 'week',\n",
       " 'say',\n",
       " 'last',\n",
       " 'love',\n",
       " 'end',\n",
       " 'live',\n",
       " 'fucking',\n",
       " 'getting',\n",
       " 'keep',\n",
       " 'talk',\n",
       " 'die',\n",
       " 'lot',\n",
       " 'ever',\n",
       " 'care',\n",
       " 'everyone',\n",
       " 'long',\n",
       " 'person',\n",
       " 'try',\n",
       " 'trying',\n",
       " 'tired',\n",
       " 'point',\n",
       " 'pain',\n",
       " 'started',\n",
       " 'hard',\n",
       " 'around',\n",
       " 'else',\n",
       " 'well',\n",
       " 'tell',\n",
       " 'school',\n",
       " 'first',\n",
       " 'find',\n",
       " 'away',\n",
       " 'happy',\n",
       " 'felt',\n",
       " 'shit',\n",
       " 'told',\n",
       " 'alone',\n",
       " 'come',\n",
       " 'made',\n",
       " 'tried',\n",
       " 'parent',\n",
       " 'said',\n",
       " 'depressed',\n",
       " 'ago',\n",
       " 'stop',\n",
       " 'sleep',\n",
       " 'maybe',\n",
       " 'worse',\n",
       " 'wish',\n",
       " 'kill',\n",
       " 'enough',\n",
       " 'give',\n",
       " 'went',\n",
       " 'reason',\n",
       " 'past',\n",
       " 'many',\n",
       " 'today',\n",
       " 'new',\n",
       " 'living',\n",
       " 'night',\n",
       " 'world',\n",
       " 'problem',\n",
       " 'home',\n",
       " 'thinking',\n",
       " 'wanted',\n",
       " 'wa',\n",
       " 'hurt',\n",
       " 'mental',\n",
       " 'health',\n",
       " 'done',\n",
       " 'actually',\n",
       " 'able',\n",
       " 'suicide',\n",
       " 'sure',\n",
       " 'self',\n",
       " 'sometimes',\n",
       " 'little',\n",
       " 'though',\n",
       " 'look',\n",
       " 'start',\n",
       " 'hope',\n",
       " 'relationship',\n",
       " 'without',\n",
       " 'best',\n",
       " 'mom',\n",
       " 'scared',\n",
       " 'left',\n",
       " 'let',\n",
       " 'hour',\n",
       " 'two',\n",
       " 'cry',\n",
       " 'old',\n",
       " 'mind',\n",
       " 'head',\n",
       " 'lost',\n",
       " 'please',\n",
       " 'kind',\n",
       " 'fuck',\n",
       " 'another',\n",
       " 'almost',\n",
       " 'place',\n",
       " 'used',\n",
       " 'suicidal',\n",
       " 'put',\n",
       " 'wrong',\n",
       " 'already',\n",
       " 'issue',\n",
       " 'stress',\n",
       " 'doctor',\n",
       " 'sad',\n",
       " 'taking',\n",
       " 'pretty',\n",
       " 'experience',\n",
       " 'might',\n",
       " 'probably',\n",
       " 'money',\n",
       " 'normal',\n",
       " 'making',\n",
       " 'guy',\n",
       " 'therapy',\n",
       " 'sorry',\n",
       " 'working',\n",
       " 'heart',\n",
       " 'leave',\n",
       " 'brain',\n",
       " 'understand',\n",
       " 'post',\n",
       " 'amp',\n",
       " 'body',\n",
       " 'symptom',\n",
       " 'med',\n",
       " 'part',\n",
       " 'next',\n",
       " 'others',\n",
       " 'im',\n",
       " 'seems',\n",
       " 'mean',\n",
       " 'high',\n",
       " 'house',\n",
       " 'change',\n",
       " 'looking',\n",
       " 'recently',\n",
       " 'finally',\n",
       " 'matter',\n",
       " 'yet',\n",
       " 'guess',\n",
       " 'bit',\n",
       " 'advice',\n",
       " 'talking',\n",
       " 'therapist',\n",
       " 'different',\n",
       " 'bed',\n",
       " 'dad',\n",
       " 'medication',\n",
       " 'social',\n",
       " 'sick',\n",
       " 'death',\n",
       " 'whole',\n",
       " 'least',\n",
       " 'gone',\n",
       " 'fear',\n",
       " 'stay',\n",
       " 'bipolar',\n",
       " 'real',\n",
       " 'found',\n",
       " 'kid',\n",
       " 'stuff',\n",
       " 'constantly',\n",
       " 'literally',\n",
       " 'took',\n",
       " 'completely',\n",
       " 'college',\n",
       " 'either',\n",
       " 'ask',\n",
       " 'idea',\n",
       " 'attack',\n",
       " 'girl',\n",
       " 'ill',\n",
       " 'deal',\n",
       " 'call',\n",
       " 'happened',\n",
       " 'great',\n",
       " 'due',\n",
       " 'remember',\n",
       " 'situation',\n",
       " 'etc',\n",
       " 'anxious',\n",
       " 'moment',\n",
       " 'move',\n",
       " 'close',\n",
       " 'read',\n",
       " 'came',\n",
       " 'side',\n",
       " 'believe',\n",
       " 'mother',\n",
       " 'future',\n",
       " 'diagnosed',\n",
       " 'seem',\n",
       " 'panic',\n",
       " 'episode',\n",
       " 'soon',\n",
       " 'okay',\n",
       " 'everyday',\n",
       " 'may',\n",
       " 'couple',\n",
       " 'become',\n",
       " 'honestly',\n",
       " 'morning',\n",
       " 'stupid',\n",
       " 'idk',\n",
       " 'wake',\n",
       " 'le',\n",
       " 'fine',\n",
       " 'saying',\n",
       " 'room',\n",
       " 'break',\n",
       " 'afraid',\n",
       " 'support',\n",
       " 'disorder',\n",
       " 'together',\n",
       " 'alive',\n",
       " 'nobody',\n",
       " 'worst',\n",
       " 'later',\n",
       " 'longer',\n",
       " 'weird',\n",
       " 'worth',\n",
       " 'happen',\n",
       " 'child',\n",
       " 'use',\n",
       " 'eat',\n",
       " 'dead',\n",
       " 'dream',\n",
       " 'far',\n",
       " 'struggling',\n",
       " 'face',\n",
       " 'hell',\n",
       " 'worried',\n",
       " 'second',\n",
       " 'fact',\n",
       " 'plan',\n",
       " 'loved',\n",
       " 'ha',\n",
       " 'god',\n",
       " 'question',\n",
       " 'thanks',\n",
       " 'starting',\n",
       " 'often',\n",
       " 'car',\n",
       " 'thank',\n",
       " 'telling',\n",
       " 'asked',\n",
       " 'full',\n",
       " 'worry',\n",
       " 'coming',\n",
       " 'state',\n",
       " 'single',\n",
       " 'word',\n",
       " 'control',\n",
       " 'big',\n",
       " 'story',\n",
       " 'turn',\n",
       " 'stuck',\n",
       " 'basically',\n",
       " 'http',\n",
       " 'emotion',\n",
       " 'eye',\n",
       " 'low',\n",
       " 'enjoy',\n",
       " 'minute',\n",
       " 'hit',\n",
       " 'man',\n",
       " 'however',\n",
       " 'decided',\n",
       " 'game',\n",
       " 'age',\n",
       " 'seeing',\n",
       " 'extremely',\n",
       " 'fun',\n",
       " 'energy',\n",
       " 'usually',\n",
       " 'needed',\n",
       " 'anyway',\n",
       " 'instead',\n",
       " 'called',\n",
       " 'mg',\n",
       " 'stopped',\n",
       " 'hospital',\n",
       " 'class',\n",
       " 'food',\n",
       " 'brother',\n",
       " 'boyfriend',\n",
       " 'lonely',\n",
       " 'barely',\n",
       " 'nice',\n",
       " 'small',\n",
       " 'cut',\n",
       " 'suck',\n",
       " 'horrible',\n",
       " 'sense',\n",
       " 'inside',\n",
       " 'girlfriend',\n",
       " 'knew',\n",
       " 'reading',\n",
       " 'mood',\n",
       " 'eating',\n",
       " 'hand',\n",
       " 'hear',\n",
       " 'currently',\n",
       " 'sound',\n",
       " 'struggle',\n",
       " 'killing',\n",
       " 'show',\n",
       " 'cause',\n",
       " 'phone',\n",
       " 'suffering',\n",
       " 'truly',\n",
       " 'pay',\n",
       " 'gotten',\n",
       " 'half',\n",
       " 'drug',\n",
       " 'father',\n",
       " 'supposed',\n",
       " 'course',\n",
       " 'woman',\n",
       " 'deserve',\n",
       " 'lately',\n",
       " 'chest',\n",
       " 'dont',\n",
       " 'human',\n",
       " 'miss',\n",
       " 'especially',\n",
       " 'sister',\n",
       " 'angry',\n",
       " 'constant',\n",
       " 'rest',\n",
       " 'possible',\n",
       " 'whatever',\n",
       " 'fucked',\n",
       " 'blood',\n",
       " 'ended',\n",
       " 'online',\n",
       " 'ok',\n",
       " 'broke',\n",
       " 'mentally',\n",
       " 'cancer',\n",
       " 'gave',\n",
       " 'hi',\n",
       " 'motivation',\n",
       " 'tomorrow',\n",
       " 'short',\n",
       " 'open',\n",
       " 'partner',\n",
       " 'fight',\n",
       " 'psychiatrist',\n",
       " 'terrible',\n",
       " 'group',\n",
       " 'outside',\n",
       " 'knowing',\n",
       " 'crazy',\n",
       " 'absolutely',\n",
       " 'test',\n",
       " 'happens',\n",
       " 'helped',\n",
       " 'lose',\n",
       " 'losing',\n",
       " 'entire',\n",
       " 'weight',\n",
       " 'empty',\n",
       " 'study',\n",
       " 'free',\n",
       " 'quite',\n",
       " 'deep',\n",
       " 'asking',\n",
       " 'saw',\n",
       " 'physical',\n",
       " 'died',\n",
       " 'spend',\n",
       " 'wanting',\n",
       " 'moved',\n",
       " 'handle',\n",
       " 'fall',\n",
       " 'spent',\n",
       " 'sort',\n",
       " 'illness',\n",
       " 'id',\n",
       " 'seen',\n",
       " 'type',\n",
       " 'worked',\n",
       " 'pill',\n",
       " 'dog',\n",
       " 'attempt',\n",
       " 'effect',\n",
       " 'share',\n",
       " 'rather',\n",
       " 'dying',\n",
       " 'forward',\n",
       " 'yesterday',\n",
       " 'play',\n",
       " 'yes',\n",
       " 'super',\n",
       " 'waiting',\n",
       " 'taken',\n",
       " 'country',\n",
       " 'whenever',\n",
       " 'hold',\n",
       " 'writing',\n",
       " 'three',\n",
       " 'bring',\n",
       " 'oh',\n",
       " 'wait',\n",
       " 'late',\n",
       " 'manic',\n",
       " 'video',\n",
       " 'severe',\n",
       " 'stressed',\n",
       " 'happiness',\n",
       " 'eventually',\n",
       " 'wondering',\n",
       " 'option',\n",
       " 'kept',\n",
       " 'failure',\n",
       " 'chance',\n",
       " 'difficult',\n",
       " 'ex',\n",
       " 'emotional',\n",
       " 'miserable',\n",
       " 'drink',\n",
       " 'ready',\n",
       " 'behind',\n",
       " 'run',\n",
       " 'sex',\n",
       " 'memory',\n",
       " 'met',\n",
       " 'easy',\n",
       " 'strong',\n",
       " 'grade',\n",
       " 'seriously',\n",
       " 'answer',\n",
       " 'trauma',\n",
       " 'walk',\n",
       " 'watch',\n",
       " 'depressive',\n",
       " 'trust',\n",
       " 'forever',\n",
       " 'given',\n",
       " 'physically',\n",
       " 'continue',\n",
       " 'write',\n",
       " 'true',\n",
       " 'sleeping',\n",
       " 'shitty',\n",
       " 'exist',\n",
       " 'interest',\n",
       " 'dealing',\n",
       " 'appointment',\n",
       " 'burden',\n",
       " 'suffer',\n",
       " 'giving',\n",
       " 'young',\n",
       " 'drive',\n",
       " 'top',\n",
       " 'th',\n",
       " 'hey',\n",
       " 'case',\n",
       " 'amount',\n",
       " 'fast',\n",
       " 'healthy',\n",
       " 'broken',\n",
       " 'reality',\n",
       " 'positive',\n",
       " 'fix',\n",
       " 'happening',\n",
       " 'early',\n",
       " 'realize',\n",
       " 'talked',\n",
       " 'act',\n",
       " 'conversation',\n",
       " 'music',\n",
       " 'mistake',\n",
       " 'daily',\n",
       " 'born',\n",
       " 'realized',\n",
       " 'ending',\n",
       " 'cold',\n",
       " 'reddit',\n",
       " 'quit',\n",
       " 'awful',\n",
       " 'stomach',\n",
       " 'damn',\n",
       " 'useless',\n",
       " 'failed',\n",
       " 'negative',\n",
       " 'lol',\n",
       " 'tonight',\n",
       " 'focus',\n",
       " 'noticed',\n",
       " 'mostly',\n",
       " 'afford',\n",
       " 'level',\n",
       " 'became',\n",
       " 'date',\n",
       " 'despite',\n",
       " 'period',\n",
       " 'treatment',\n",
       " 'abuse',\n",
       " 'sit',\n",
       " 'leaving',\n",
       " 'similar',\n",
       " 'turned',\n",
       " 'moving',\n",
       " 'covid',\n",
       " 'step',\n",
       " 'check',\n",
       " 'result',\n",
       " 'stand',\n",
       " 'cope',\n",
       " 'attention',\n",
       " 'term',\n",
       " 'exhausted',\n",
       " 'peace',\n",
       " 'piece',\n",
       " 'important',\n",
       " 'slowly',\n",
       " 'fault',\n",
       " 'experienced',\n",
       " 'name',\n",
       " 'wife',\n",
       " 'numb',\n",
       " 'meet',\n",
       " 'mine',\n",
       " 'along',\n",
       " 'vent',\n",
       " 'watching',\n",
       " 'upset',\n",
       " 'explain',\n",
       " 'worthless',\n",
       " 'mess',\n",
       " 'drinking',\n",
       " 'heard',\n",
       " 'dark',\n",
       " 'decision',\n",
       " 'harm',\n",
       " 'university',\n",
       " 'selfish',\n",
       " 'cant',\n",
       " 'pressure',\n",
       " 'childhood',\n",
       " 'yeah',\n",
       " 'husband',\n",
       " 'set',\n",
       " 'career',\n",
       " 'front',\n",
       " 'listen',\n",
       " 'lie',\n",
       " 'changed',\n",
       " 'bc',\n",
       " 'light',\n",
       " 'none',\n",
       " 'hang',\n",
       " 'anyways',\n",
       " 'serious',\n",
       " 'note',\n",
       " 'playing',\n",
       " 'multiple',\n",
       " 'somehow',\n",
       " 'major',\n",
       " 'towards',\n",
       " 'must',\n",
       " 'except',\n",
       " 'lack',\n",
       " 'wonder',\n",
       " 'sadness',\n",
       " 'whether',\n",
       " 'reach',\n",
       " 'diagnosis',\n",
       " 'buy',\n",
       " 'hello',\n",
       " 'forget',\n",
       " 'several',\n",
       " 'com',\n",
       " 'calm',\n",
       " 'water',\n",
       " 'huge',\n",
       " 'suddenly',\n",
       " 'harder',\n",
       " 'terrified',\n",
       " 'goal',\n",
       " 'ugly',\n",
       " 'hobby',\n",
       " 'genuinely',\n",
       " 'birthday',\n",
       " 'co',\n",
       " 'hoping',\n",
       " 'abusive',\n",
       " 'painful',\n",
       " 'woke',\n",
       " 'adult',\n",
       " 'caused',\n",
       " 'student',\n",
       " 'alcohol',\n",
       " 'speak',\n",
       " 'general',\n",
       " 'personality',\n",
       " 'summer',\n",
       " 'looked',\n",
       " 'anybody',\n",
       " 'effort',\n",
       " 'sitting',\n",
       " 'contact',\n",
       " 'helping',\n",
       " 'cycle',\n",
       " 'finding',\n",
       " 'hopeless',\n",
       " 'emotionally',\n",
       " 'nervous',\n",
       " 'hurting',\n",
       " 'middle',\n",
       " 'antidepressant',\n",
       " 'figure',\n",
       " 'definitely',\n",
       " 'joke',\n",
       " 'using',\n",
       " 'text',\n",
       " 'anger',\n",
       " 'guilty',\n",
       " 'clean',\n",
       " 'trigger',\n",
       " 'imagine',\n",
       " 'order',\n",
       " 'putting',\n",
       " 'tip',\n",
       " 'apart',\n",
       " 'older',\n",
       " 'keeping',\n",
       " 'easier',\n",
       " 'hair',\n",
       " 'trouble',\n",
       " 'although',\n",
       " 'choice',\n",
       " 'simply',\n",
       " 'escape',\n",
       " 'voice',\n",
       " 'accept',\n",
       " 'form',\n",
       " 'message',\n",
       " 'cat',\n",
       " 'purpose',\n",
       " 'somewhere',\n",
       " 'friendship',\n",
       " 'mad',\n",
       " 'fighting',\n",
       " 'apartment',\n",
       " 'safe',\n",
       " 'random',\n",
       " 'degree',\n",
       " 'arm',\n",
       " 'adhd',\n",
       " 'passed',\n",
       " 'asleep',\n",
       " 'dating',\n",
       " 'society',\n",
       " 'headache',\n",
       " 'learn',\n",
       " 'avpd',\n",
       " 'book',\n",
       " 'waste',\n",
       " 'avoid',\n",
       " 'blame',\n",
       " 'falling',\n",
       " 'push',\n",
       " 'comment',\n",
       " 'notice',\n",
       " 'pas',\n",
       " 'lazy',\n",
       " 'weak',\n",
       " 'badly',\n",
       " 'within',\n",
       " 'fail',\n",
       " 'existence',\n",
       " 'medical',\n",
       " 'clear',\n",
       " 'na',\n",
       " 'straight',\n",
       " 'pathetic',\n",
       " 'complete',\n",
       " 'weekend',\n",
       " 'regret',\n",
       " 'convinced',\n",
       " 'obviously',\n",
       " 'mania',\n",
       " 'space',\n",
       " 'lived',\n",
       " 'sent',\n",
       " 'sign',\n",
       " 'account',\n",
       " 'event',\n",
       " 'impossible',\n",
       " 'bother',\n",
       " 'city',\n",
       " 'related',\n",
       " 'honest',\n",
       " 'likely',\n",
       " 'funny',\n",
       " 'certain',\n",
       " 'current',\n",
       " 'fake',\n",
       " 'joy',\n",
       " 'perfect',\n",
       " 'unable',\n",
       " 'process',\n",
       " 'ptsd',\n",
       " 'mum',\n",
       " 'personal',\n",
       " 'relate',\n",
       " 'toxic',\n",
       " 'drunk',\n",
       " 'amazing',\n",
       " 'exactly',\n",
       " 'meant',\n",
       " 'example',\n",
       " 'manage',\n",
       " 'movie',\n",
       " 'save',\n",
       " 'running',\n",
       " 'lead',\n",
       " 'becoming',\n",
       " 'fit',\n",
       " 'area',\n",
       " 'disease',\n",
       " 'loss',\n",
       " 'stable',\n",
       " 'hanging',\n",
       " 'ruined',\n",
       " 'he',\n",
       " 'waking',\n",
       " 'weed',\n",
       " 'worrying',\n",
       " 'system',\n",
       " 'dr',\n",
       " 'comfort',\n",
       " 'comfortable',\n",
       " 'confused',\n",
       " 'treat',\n",
       " 'rant',\n",
       " 'insurance',\n",
       " 'skin',\n",
       " 'neck',\n",
       " 'till',\n",
       " 'experiencing',\n",
       " 'killed',\n",
       " 'immediately',\n",
       " 'aware',\n",
       " 'seemed',\n",
       " 'married',\n",
       " 'hole',\n",
       " 'skill',\n",
       " 'son',\n",
       " 'extreme',\n",
       " 'driving',\n",
       " 'beautiful',\n",
       " 'proud',\n",
       " 'exam',\n",
       " 'daughter',\n",
       " 'guilt',\n",
       " 'leg',\n",
       " 'tear',\n",
       " 'internet',\n",
       " 'condition',\n",
       " 'gun',\n",
       " 'planning',\n",
       " 'nearly',\n",
       " 'poor',\n",
       " 'member',\n",
       " 'commit',\n",
       " 'ive',\n",
       " 'interested',\n",
       " 'throat',\n",
       " 'easily',\n",
       " 'male',\n",
       " 'busy',\n",
       " 'dumb',\n",
       " 'door',\n",
       " 'posting',\n",
       " 'anywhere',\n",
       " 'professional',\n",
       " 'younger',\n",
       " 'forced',\n",
       " 'psych',\n",
       " 'community',\n",
       " 'exercise',\n",
       " 'near',\n",
       " 'appreciate',\n",
       " 'teacher',\n",
       " 'simple',\n",
       " 'response',\n",
       " 'medium',\n",
       " 'twice',\n",
       " 'function',\n",
       " 'medicine',\n",
       " 'doubt',\n",
       " 'listening',\n",
       " 'laugh',\n",
       " 'bill',\n",
       " 'throughout',\n",
       " 'causing',\n",
       " 'totally',\n",
       " 'force',\n",
       " 'expect',\n",
       " 'desire',\n",
       " 'line',\n",
       " 'quickly',\n",
       " 'cried',\n",
       " 'baby',\n",
       " 'mention',\n",
       " 'hopefully',\n",
       " 'throw',\n",
       " 'growing',\n",
       " 'beat',\n",
       " 'company',\n",
       " 'nightmare',\n",
       " 'intense',\n",
       " 'begin',\n",
       " 'incredibly',\n",
       " 'breath',\n",
       " 'nowhere',\n",
       " 'walking',\n",
       " 'prescribed',\n",
       " 'trapped',\n",
       " 'picture',\n",
       " 'known',\n",
       " 'smile',\n",
       " 'lay',\n",
       " 'loving',\n",
       " 'opportunity',\n",
       " 'loneliness',\n",
       " 'brought',\n",
       " 'managed',\n",
       " 'treated',\n",
       " 'appreciated',\n",
       " 'survive',\n",
       " 'dose',\n",
       " 'list',\n",
       " 'overwhelmed',\n",
       " 'struggled',\n",
       " 'shut',\n",
       " 'uncomfortable',\n",
       " 'freaking',\n",
       " 'excited',\n",
       " 'trip',\n",
       " 'party',\n",
       " 'action',\n",
       " 'visit',\n",
       " 'began',\n",
       " 'pandemic',\n",
       " 'lying',\n",
       " 'sub',\n",
       " 'kinda',\n",
       " 'bullshit',\n",
       " 'normally',\n",
       " 'loser',\n",
       " 'gonna',\n",
       " 'number',\n",
       " 'staying',\n",
       " 'breathing',\n",
       " 'everytime',\n",
       " 'finish',\n",
       " 'add',\n",
       " 'non',\n",
       " 'pick',\n",
       " 'shower',\n",
       " 'fell',\n",
       " 'learned',\n",
       " 'holding',\n",
       " 'wall',\n",
       " 'beginning',\n",
       " 'pointless',\n",
       " 'task',\n",
       " 'meaning',\n",
       " 'fully',\n",
       " 'meeting',\n",
       " 'anti',\n",
       " 'as',\n",
       " 'foot',\n",
       " 'apparently',\n",
       " 'boring',\n",
       " 'breakdown',\n",
       " 'ocd',\n",
       " 'existing',\n",
       " 'common',\n",
       " 'somebody',\n",
       " 'bunch',\n",
       " 'boy',\n",
       " 'soul',\n",
       " 'scary',\n",
       " 'offer',\n",
       " 'breaking',\n",
       " 'allowed',\n",
       " 'affect',\n",
       " 'possibly',\n",
       " 'drop',\n",
       " 'session',\n",
       " 'ruin',\n",
       " 'solution',\n",
       " 'sexual',\n",
       " 'coping',\n",
       " 'truth',\n",
       " 'send',\n",
       " 'history',\n",
       " 'stressful',\n",
       " 'cool',\n",
       " 'missed',\n",
       " 'sibling',\n",
       " 'missing',\n",
       " 'tho',\n",
       " 'unless',\n",
       " 'follow',\n",
       " 'urge',\n",
       " 'annoying',\n",
       " 'considering',\n",
       " 'goodbye',\n",
       " 'abused',\n",
       " 'title',\n",
       " 'actual',\n",
       " 'progress',\n",
       " 'confidence',\n",
       " 'connection',\n",
       " 'cared',\n",
       " 'touch',\n",
       " 'pm',\n",
       " 'everybody',\n",
       " 'ability',\n",
       " 'lucky',\n",
       " 'heavy',\n",
       " 'relief',\n",
       " 'rate',\n",
       " 'hide',\n",
       " 'behavior',\n",
       " 'wrote',\n",
       " 'chronic',\n",
       " 'choose',\n",
       " 'failing',\n",
       " 'four',\n",
       " 'final',\n",
       " ...]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_w2v.wv.index_to_key  #to check the vocabulary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3975236c-0e4c-421a-8138-56e6546281ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('degree', 0.8473808169364929),\n",
       " ('field', 0.8235071897506714),\n",
       " ('academic', 0.8171334862709045),\n",
       " ('scholarship', 0.8151994943618774),\n",
       " ('student', 0.8049637675285339),\n",
       " ('finance', 0.8041580319404602),\n",
       " ('engineering', 0.802412211894989),\n",
       " ('bachelor', 0.7974151372909546),\n",
       " ('career', 0.788594126701355),\n",
       " ('salary', 0.7884148359298706)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_w2v.wv.similar_by_word('education')   #function to check the similarty (sementic word) word in the given text data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "054afcc3-497b-4384-b196-d9ce9944cf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now creating function for  Average word2vec for simpler model\n",
    "def avg_word2vec(doc, model_w2v, vector_size):\n",
    "    valid_words = [model_w2v.wv[word] for word in doc if word in model_w2v.wv.index_to_key]\n",
    "    if len(valid_words) > 0:\n",
    "        return np.mean(valid_words, axis=0)\n",
    "    else:\n",
    "        # Return a zero vector if no valid words are found\n",
    "        return np.zeros(vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "872410ec-2ecf-42d6-96d9-43128979eeef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 52497/52497 [01:20<00:00, 648.15it/s] \n"
     ]
    }
   ],
   "source": [
    "#apply Average Word2vec for the entire corpus\n",
    "X = []\n",
    "vector_size = model_w2v.vector_size  # Get the size of the word2vec vectors\n",
    "for i in tqdm(range(len(words))):\n",
    "    X.append(avg_word2vec(words[i], model_w2v, vector_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06294712-026f-4b3c-9531-ed4c9189bbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X)\n",
    "y = df_filtered['status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c7a4512-736e-47e1-b92d-babc2563603c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Test Split\n",
    "w2v_X_train , w2v_X_test, w2v_y_train, w2v_y_test = train_test_split(X, y, test_size=0.2, random_state = 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34406030-c5b3-47a7-873d-9d20817c4e27",
   "metadata": {},
   "source": [
    "# Modelling\n",
    "**Random Forest Classifier with default parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9fca8885-29e3-4c87-9ecd-4ec65ba685b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy 0.9772364692716147\n",
      "Test Accuracy 0.685047619047619\n",
      "Classification Report:\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "             Anxiety       0.81      0.60      0.69       811\n",
      "             Bipolar       0.75      0.41      0.53       538\n",
      "          Depression       0.56      0.71      0.63      2993\n",
      "              Normal       0.79      0.94      0.86      3332\n",
      "Personality disorder       1.00      0.24      0.38       216\n",
      "              Stress       0.92      0.21      0.34       514\n",
      "            Suicidal       0.64      0.51      0.56      2096\n",
      "\n",
      "            accuracy                           0.69     10500\n",
      "           macro avg       0.78      0.52      0.57     10500\n",
      "        weighted avg       0.70      0.69      0.67     10500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "Emotion_classifier_model_W2V = RandomForestClassifier(min_samples_split=10)\n",
    "Emotion_classifier_model_W2V.fit(w2v_X_train,w2v_y_train)\n",
    "\n",
    "#Prediction\n",
    "ypred_test = Emotion_classifier_model_W2V.predict(w2v_X_test)\n",
    "ypred_train = Emotion_classifier_model_W2V.predict(w2v_X_train)\n",
    "\n",
    "# Evaluation\n",
    "\n",
    "print(\"Train Accuracy\", accuracy_score(w2v_y_train,ypred_train))\n",
    "print(\"Test Accuracy\", accuracy_score(w2v_y_test,ypred_test))\n",
    "report = classification_report(w2v_y_test, ypred_test)\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec1e712c-3c8d-4ac9-bd03-f3388cbe68aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your new sentence here:  Tired of living daily like everything okay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anxiety: 4.82%\n",
      "Bipolar: 3.47%\n",
      "Depression: 34.53%\n",
      "Normal: 13.97%\n",
      "Personality disorder: 0.83%\n",
      "Stress: 5.78%\n",
      "Suicidal: 36.61%\n",
      "Prediction probabilities of your current mental state is  : ['Suicidal']\n"
     ]
    }
   ],
   "source": [
    "# Trying it on the new text\n",
    "\n",
    "# New sentence to classify\n",
    "new_sentence = input(\"Enter your new sentence here: \")\n",
    "\n",
    "# Preprocess the new sentence\n",
    "def preprocess_single_sentence(text):\n",
    "    'Applying lemmetization and removing english stop words'\n",
    "    cleaned_text = re.sub('[^a-zA-Z]', \" \", text)\n",
    "    cleaned_text = cleaned_text.lower()\n",
    "    words = cleaned_text.split()\n",
    "    processed_words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    processed_text = \" \".join(processed_words)\n",
    "    return processed_text\n",
    "\n",
    "processed_sentence = preprocess_single_sentence(new_sentence)\n",
    "\n",
    "processed_sentence = simple_preprocess(processed_sentence, min_len=2, max_len=15)\n",
    "\n",
    "# Step 3: Transform the sentence using the avg_word2vec function\n",
    "new_sentence_vector = avg_word2vec(processed_sentence, model_w2v, vector_size).reshape(1, -1)  # Reshape to 2D array\n",
    "\n",
    "# Make prediction using the trained model\n",
    "prediction = Emotion_classifier_model_W2V.predict(new_sentence_vector)\n",
    "prediction_proba = Emotion_classifier_model_W2V.predict_proba(new_sentence_vector)\n",
    "\n",
    "# Map prediction to emotions\n",
    "mental_state = {\n",
    "    0: \"Anxiety\",\n",
    "    1: \"Bipolar\",\n",
    "    2: \"Depression\",\n",
    "    3: \"Normal\",\n",
    "    4: \"Personality disorder\",\n",
    "    5: \"Stress\",\n",
    "    6: \"Suicidal\"\n",
    "}\n",
    "\n",
    "for index, proba in enumerate(prediction_proba[0]):\n",
    "    emotion = mental_state[index]\n",
    "    print(f\"{emotion}: {proba * 100:.2f}%\")\n",
    "print(\"Prediction probabilities of your current mental state is  :\", prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd63e9c-a59b-4a01-857b-e73886a5cfb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
