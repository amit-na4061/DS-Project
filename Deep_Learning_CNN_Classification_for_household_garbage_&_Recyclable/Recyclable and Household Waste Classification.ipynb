{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b3b8f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing important libraries to work on the image classification problem\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "944cf2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale = 1/255,\n",
    "                                  shear_range = 0.2,\n",
    "                                  zoom_range = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0dc4403a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7250 images belonging to 29 classes.\n"
     ]
    }
   ],
   "source": [
    "training_set = train_datagen.flow_from_directory('images/Train',\n",
    "                                                target_size =(128,128),\n",
    "                                                 class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1dc1b63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aerosol_cans': 0,\n",
       " 'aluminum_food_cans': 1,\n",
       " 'aluminum_soda_cans': 2,\n",
       " 'cardboard_boxes': 3,\n",
       " 'cardboard_packaging': 4,\n",
       " 'clothing': 5,\n",
       " 'disposable_plastic_cutlery': 6,\n",
       " 'eggshells': 7,\n",
       " 'food_waste': 8,\n",
       " 'glass_beverage_bottles': 9,\n",
       " 'glass_cosmetic_containers': 10,\n",
       " 'glass_food_jars': 11,\n",
       " 'magazines': 12,\n",
       " 'newspaper': 13,\n",
       " 'office_paper': 14,\n",
       " 'paper_cups': 15,\n",
       " 'plastic_cup_lids': 16,\n",
       " 'plastic_detergent_bottles': 17,\n",
       " 'plastic_food_containers': 18,\n",
       " 'plastic_shopping_bags': 19,\n",
       " 'plastic_soda_bottles': 20,\n",
       " 'plastic_straws': 21,\n",
       " 'plastic_trash_bags': 22,\n",
       " 'plastic_water_bottles': 23,\n",
       " 'shoes': 24,\n",
       " 'steel_food_cans': 25,\n",
       " 'styrofoam_cups': 26,\n",
       " 'styrofoam_food_containers': 27,\n",
       " 'tea_bags': 28}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b48a469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1450 images belonging to 29 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(rescale = 1/255)\n",
    "test_set = train_datagen.flow_from_directory('images/Test',\n",
    "                                                target_size =(128,128),\n",
    "                                                 class_mode = 'categorical')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8946de",
   "metadata": {},
   "source": [
    "# Modelling the Convolution Nural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ffa1060",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">127008</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │    <span style=\"color: #00af00; text-decoration-color: #00af00\">32,514,304</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,741</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m127008\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │    \u001b[38;5;34m32,514,304\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m29\u001b[0m)             │         \u001b[38;5;34m3,741\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">32,551,837</span> (124.18 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m32,551,837\u001b[0m (124.18 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">32,551,837</span> (124.18 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m32,551,837\u001b[0m (124.18 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Initializing CNN\n",
    "Classifier = Sequential()\n",
    "\n",
    "#Step 1 - Convolution\n",
    "Classifier.add(Conv2D(input_shape = [128, 128, 3],\n",
    "                     filters = 32,\n",
    "                     kernel_size=3,\n",
    "                     activation='relu'))\n",
    "#Step 2 - Maxpooling\n",
    "Classifier.add(MaxPooling2D(pool_size=2,strides=2))\n",
    "\n",
    "#Step 3 - Flattening \n",
    "Classifier.add(Flatten())\n",
    "\n",
    "#Step 4 - Full Connection \n",
    "\n",
    "Classifier.add(Dense(256, activation='relu'))  \n",
    "Classifier.add(Dense(128, activation='relu'))  \n",
    "               \n",
    "\n",
    "# Final output layer with number of units equal to number of classes\n",
    "num_classes = 29  \n",
    "Classifier.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "Classifier.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Summary of the model\n",
    "Classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45bbe2e9-4cce-41b8-8689-fe1fff0efb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the EarlyStopping callback\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35844091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m227/227\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m206s\u001b[0m 872ms/step - accuracy: 0.0773 - loss: 5.4665 - val_accuracy: 0.1890 - val_loss: 2.9278\n",
      "Epoch 2/10\n",
      "\u001b[1m227/227\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 514ms/step - accuracy: 0.2663 - loss: 2.6378 - val_accuracy: 0.2683 - val_loss: 2.5369\n",
      "Epoch 3/10\n",
      "\u001b[1m227/227\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 507ms/step - accuracy: 0.3680 - loss: 2.2445 - val_accuracy: 0.3710 - val_loss: 2.2205\n",
      "Epoch 4/10\n",
      "\u001b[1m227/227\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 492ms/step - accuracy: 0.4407 - loss: 1.9241 - val_accuracy: 0.4041 - val_loss: 2.0657\n",
      "Epoch 5/10\n",
      "\u001b[1m227/227\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 508ms/step - accuracy: 0.4919 - loss: 1.7688 - val_accuracy: 0.4848 - val_loss: 1.7935\n",
      "Epoch 6/10\n",
      "\u001b[1m227/227\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 497ms/step - accuracy: 0.5316 - loss: 1.6220 - val_accuracy: 0.4821 - val_loss: 1.7781\n",
      "Epoch 7/10\n",
      "\u001b[1m227/227\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 487ms/step - accuracy: 0.5711 - loss: 1.5093 - val_accuracy: 0.5524 - val_loss: 1.5280\n",
      "Epoch 8/10\n",
      "\u001b[1m227/227\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 498ms/step - accuracy: 0.6089 - loss: 1.3342 - val_accuracy: 0.5793 - val_loss: 1.4564\n",
      "Epoch 9/10\n",
      "\u001b[1m227/227\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 475ms/step - accuracy: 0.6301 - loss: 1.2688 - val_accuracy: 0.6069 - val_loss: 1.3257\n",
      "Epoch 10/10\n",
      "\u001b[1m227/227\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 491ms/step - accuracy: 0.6611 - loss: 1.1496 - val_accuracy: 0.6386 - val_loss: 1.2503\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x22738dc4ed0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Classifier.fit( x = training_set, validation_data = test_set, epochs=10,callbacks=[early_stopping],validation_split=0.1,batch_size =5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845c5e3e",
   "metadata": {},
   "source": [
    "# Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f80cef3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 216ms/step\n",
      "Given image belongs to 'glass_food_jars'.\n"
     ]
    }
   ],
   "source": [
    "#Load the data\n",
    "\n",
    "test_image = Image.open(\"images/Prediction check/Image_84.png\")\n",
    "\n",
    "#Data Preprocessing\n",
    "\n",
    "test_image = test_image.resize((128,128))\n",
    "test_image = np.array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis=0)\n",
    "\n",
    "\n",
    "#Prediction \n",
    "\n",
    "Result = Classifier.predict(test_image)\n",
    "for key, val in training_set.class_indices.items():\n",
    "        if val == np.argmax(Result):\n",
    "            print(f\"Given image belongs to '{key}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dae11217",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import model_from_json\n",
    "import h5py\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3543cb4d-61a0-4047-8ffa-fe9e36da6d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classifier.save('my_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc2273c8-fb65-400f-bdb1-96d0825e2821",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Perform sample inferences on random test images with different labels\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#Classifier.eval()\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m      4\u001b[0m     indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(test_dataset)))\n\u001b[0;32m      5\u001b[0m     random\u001b[38;5;241m.\u001b[39mshuffle(indices)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "# Perform sample inferences on random test images with different labels\n",
    "Classifier.eval()\n",
    "with torch.no_grad():\n",
    "    indices = list(range(len(test_dataset)))\n",
    "    random.shuffle(indices)\n",
    "    \n",
    "    selected_images = []\n",
    "    selected_labels = []\n",
    "    selected_predicted = []\n",
    "    \n",
    "    for index in indices:\n",
    "        image, label = test_dataset[index]\n",
    "        image = image.unsqueeze(0).to('cuda')\n",
    "        \n",
    "        output = model(image)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        \n",
    "        if label not in selected_labels:\n",
    "            selected_images.append(image)\n",
    "            selected_labels.append(label)\n",
    "            selected_predicted.append(predicted.item())\n",
    "        \n",
    "        if len(selected_labels) == 9:\n",
    "            break\n",
    "    \n",
    "    fig, axes = plt.subplots(3, 3, figsize=(12, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i in range(9):\n",
    "        axes[i].imshow(selected_images[i].squeeze().cpu().permute(1, 2, 0))\n",
    "        axes[i].set_title(f\"True: {train_dataset.classes[selected_labels[i]]}\\nPredicted: {train_dataset.classes[selected_predicted[i]]}\")\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7ec1c9-729d-4381-8624-10ed0b5e5c0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
